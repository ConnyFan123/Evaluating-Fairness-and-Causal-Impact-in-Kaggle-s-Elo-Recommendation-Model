{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3RvVmS_OK2L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "BJv4o3XjKXaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuI9ZrxkOYA0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBjB2zDyOeKk"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions download -c elo-merchant-category-recommendation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mim7eHhYOjii"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"elo-merchant-category-recommendation.zip\", 'r') as zip_ref:\n",
        "    #zip_ref.extractall(\"talkingdata_adtracking\")\n",
        "\n",
        "!ls talkingdata_adtracking"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start from here we run the code of the ADS."
      ],
      "metadata": {
        "id": "8WtDZRnUgw35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "import warnings\n",
        "import time\n",
        "import sys\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "pd.set_option('display.max_columns', 500)"
      ],
      "metadata": {
        "id": "UFKy58L8VFkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1 Data Loading and Memory Optimization**\n",
        "\n",
        "First, we load the Elo Merchant Category Recommendation dataset and apply a memory reduction function to optimize memory usage. The function reduce_mem_usage downcasts numeric columns to more efficient types where possible. This helps speed up processing and avoids running out of memory when dealing with the large transaction tables."
      ],
      "metadata": {
        "id": "EnDG3GCYcLhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy lightgbm xgboost catboost fairlearn shap\n",
        "\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        if col_type in [np.object_, object]:\n",
        "            # Skip object types (e.g., strings) for downcasting\n",
        "            continue\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            # Downcast integers to smallest possible subdtype\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                # (If data fits in int64, we leave as is)\n",
        "            # Downcast floats to float32/16 if possible without losing precision\n",
        "            else:\n",
        "                if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                # (If outside float32 range, keep as float64)\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose:\n",
        "        print(f'Memory usage decreased to {end_mem:5.2f} MB '\n",
        "              f'({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n",
        "    return df"
      ],
      "metadata": {
        "id": "qHL4uw6XbcLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code above, after downloading and extracting the dataset, we define reduce_mem_usage to convert numeric columns to smaller types where possible. This function will be applied to the transaction data tables to significantly cut down memory usage (often a reduction of 25-50% or more)."
      ],
      "metadata": {
        "id": "D6qpwBfAdqL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2 Data Cleaning and Basic Feature Engineering**\n",
        "\n",
        "Next, we load the transaction datasets and perform data cleaning steps such as parsing dates, handling missing values, and converting categorical flags to binary. We also create initial features like the time difference between first activation and a reference date."
      ],
      "metadata": {
        "id": "UP2-BFpTdzJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_transactions = pd.read_csv('talkingdata_adtracking/new_merchant_transactions.csv',\n",
        "                               parse_dates=['purchase_date'])\n",
        "\n",
        "historical_transactions = pd.read_csv('talkingdata_adtracking/historical_transactions.csv',\n",
        "                                      parse_dates=['purchase_date'])\n",
        "\n",
        "# Convert boolean-like flags from 'Y'/'N' to 1/0\n",
        "def binarize(df):\n",
        "    for col in ['authorized_flag', 'category_1']:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].map({'Y': 1, 'N': 0})\n",
        "    return df\n",
        "\n",
        "historical_transactions = binarize(historical_transactions)\n",
        "new_transactions = binarize(new_transactions)\n",
        "\n",
        "\n",
        "train = pd.read_csv('train.csv', parse_dates=['first_active_month'])\n",
        "test = pd.read_csv('test.csv', parse_dates=['first_active_month'])\n",
        "train['elapsed_time'] = (pd.to_datetime('2018-02-01') - train['first_active_month']).dt.days\n",
        "test['elapsed_time'] = (pd.to_datetime('2018-02-01') - test['first_active_month']).dt.days\n",
        "\n",
        "# Separate target from train data\n",
        "target = train['target']\n",
        "#train.drop('target', axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "0J1XQh78dTcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load historical_transactions and new_transactions, which contain transaction history for each card (card_id). The purchase_date column is parsed as datetime.\n",
        "We define a helper binarize to map categorical flags authorized_flag (which indicates if a historical transaction was approved) and category_1 from 'Y'/'N' to 1/0. This makes them numeric binary features.\n",
        "\n",
        "We load train.csv and test.csv, which contain one row per card with some basic features (feature_1, feature_2, feature_3 are anonymized card attributes, and first_active_month when the card was first used). We parse first_active_month as a date.\n",
        "\n",
        "\n",
        "We create an elapsed_time feature measuring the number of days between the card's first active month and a reference date (Feb 1, 2018, which is around the dataset cutoff). This captures card age (older cards will have larger elapsed_time).\n",
        "\n",
        "At this point, basic data cleaning is done: missing values in first_active_month (if any) would result in elapsed_time being NaN which we could fill with a default (though in this dataset there were no missing first_active_month in train/test). The flag conversions handle boolean fields. We will next engineer more features from the transaction history."
      ],
      "metadata": {
        "id": "gPeC901xfD4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature engineering**\n",
        "\n",
        "The ADS added the following features followed the result of [this kernel](https://www.kaggle.com/code/chauhuynh/my-first-kernel-3-699)"
      ],
      "metadata": {
        "id": "4YKLgFOMis9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3 Feature Engineering from Transaction History**\n",
        "\n",
        "We will aggregate the transaction data to create features at the card level. This involves generating summary statistics from both the historical transactions (including authorized and unauthorized transactions) and the new merchant transactions.\n",
        "\n",
        "Steps include:\n",
        "\n",
        "\n",
        "Date-based features: We compute how long ago each transaction happened relative to a reference (today or a fixed date) and combine with the provided month_lag (the month index relative to the reference date in the dataset).\n",
        "One-hot encoding categorical features: The transaction tables have categorical fields category_2 and category_3. We create one-hot encoded dummy variables for these to use in aggregation.\n",
        "\n",
        "\n",
        "Aggregations: For each card, we calculate aggregated metrics (sum, mean, std, etc.) of transaction amounts, installment counts, and other fields, both overall and grouped by certain categories (like by month lag, or by whether the transaction was authorized).\n",
        "\n",
        "\n",
        "Separating authorized transactions: We split historical transactions into authorized and unauthorized sets to derive separate features from each, as their patterns might influence loyalty differently.\n",
        "\n",
        "\n",
        "Successive transaction aggregates: We create some advanced features capturing the relationship between two fields (e.g., how purchase amounts vary by category or city for each card).\n"
      ],
      "metadata": {
        "id": "F-Lsz6tTfweM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "# Create 'month_diff' feature: how many months since the purchase\n",
        "historical_transactions['month_diff'] = ((datetime.datetime.today() - historical_transactions['purchase_date']).dt.days // 30)\n",
        "historical_transactions['month_diff'] += historical_transactions['month_lag']\n",
        "\n",
        "new_transactions['month_diff'] = ((datetime.datetime.today() - new_transactions['purchase_date']).dt.days // 30)\n",
        "new_transactions['month_diff'] += new_transactions['month_lag']\n",
        "\n",
        "historical_transactions = pd.get_dummies(historical_transactions, columns=['category_2', 'category_3'])\n",
        "new_transactions = pd.get_dummies(new_transactions, columns=['category_2', 'category_3'])\n",
        "\n",
        "# Reduce memory usage\n",
        "historical_transactions = reduce_mem_usage(historical_transactions, verbose=True)\n",
        "new_transactions = reduce_mem_usage(new_transactions, verbose=True)\n",
        "\n",
        "# Aggregation: authorized_flag mean per card (what fraction of transactions were authorized)\n",
        "agg_auth = historical_transactions.groupby('card_id')['authorized_flag'].mean().reset_index()\n",
        "agg_auth.columns = ['card_id', 'authorized_flag_mean']  # fraction of transactions authorized for each card\n",
        "\n",
        "# Split historical transactions\n",
        "authorized_transactions = historical_transactions[historical_transactions['authorized_flag'] == 1]\n",
        "unauthorized_transactions = historical_transactions[historical_transactions['authorized_flag'] == 0]\n",
        "\n",
        "# Add a purchase month feature (month of year) for potential seasonality\n",
        "for df in [authorized_transactions, unauthorized_transactions, new_transactions]:\n",
        "    df['purchase_month'] = df['purchase_date'].dt.month\n"
      ],
      "metadata": {
        "id": "e2owEOTWfvFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We compute month_diff for both historical and new transactions, which estimates how many months ago the transaction occurred. It uses purchase_date and current date (as a stand-in for a consistent reference) and adjusts by month_lag (a feature given in the dataset indicating how far back each historical transaction is relative to the reference month for that card). This feature can capture recency of transactions.\n",
        "\n",
        "\n",
        "We apply one-hot encoding to category_2 and category_3 in the transaction data. These were categorical variables with a small number of possible values, now turned into dummy binary columns (e.g., category_2_1.0, category_2_2.0, ..., category_3_A, category_3_B, etc.). This allows us to compute meaningful aggregates like the percentage of transactions of each category for a card.\n",
        "We use reduce_mem_usage on the expanded transaction tables, since one-hot encoding can increase the number of columns significantly.\n",
        "\n",
        "\n",
        "We then derive authorized_flag_mean for each card as the mean of the authorized_flag column in historical transactions. This gives the proportion of transactions that were approved (Y) for each card. This could be indicative of card usage stability or credit issues (cards with lower authorization rates might be experiencing more declines).\n",
        "\n",
        "\n",
        "We separate the historical_transactions into two DataFrames: one for authorized transactions and one for unauthorized (declined) transactions. We will aggregate features from these separately to see if they have different predictive power.\n",
        "\n",
        "\n",
        "We add a purchase_month feature to capture the month of year when the purchase happened, which could detect seasonal usage patterns (for example, higher spending in December holidays, etc.).\n",
        "\n",
        "\n",
        "Next, we define an aggregation function to compute a variety of features for each card from a given transaction table. We will apply this to the authorized transactions, unauthorized (historical) transactions, and new transactions."
      ],
      "metadata": {
        "id": "5lvJemV3gM34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an aggregation function for transaction history\n",
        "def aggregate_transactions(history_df):\n",
        "    history_df.loc[:, 'purchase_date'] = pd.DatetimeIndex(history_df['purchase_date']).astype(np.int64) * 1e-9\n",
        "    agg_func = {\n",
        "        'authorized_flag': ['sum', 'mean'],\n",
        "        'category_1': ['sum', 'mean'],\n",
        "        **{col: ['mean'] for col in history_df.columns if col.startswith('category_2_') or col.startswith('category_3_')},\n",
        "        'merchant_id': ['nunique'],\n",
        "        'merchant_category_id': ['nunique'],\n",
        "        'state_id': ['nunique'],\n",
        "        'city_id': ['nunique'],\n",
        "        'subsector_id': ['nunique'],\n",
        "        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n",
        "        'installments': ['sum', 'mean', 'max', 'min', 'std'],\n",
        "        'purchase_month': ['mean', 'max', 'min', 'std'],\n",
        "        'purchase_date': [np.ptp, 'min', 'max'],\n",
        "        'month_lag': ['mean', 'max', 'min', 'std'],\n",
        "        'month_diff': ['mean']\n",
        "    }\n",
        "    # Perform groupby aggregation by card_id\n",
        "    agg_df = history_df.groupby('card_id').agg(agg_func)\n",
        "    # Flatten multi-level column index resulting from aggregation\n",
        "    agg_df.columns = ['_'.join(col).strip() for col in agg_df.columns.values]\n",
        "    agg_df.reset_index(inplace=True)\n",
        "    # Also count the number of transactions per card\n",
        "    count_df = history_df.groupby('card_id').size().reset_index(name='transactions_count')\n",
        "    # Merge count into agg_df\n",
        "    agg_df = agg_df.merge(count_df, on='card_id', how='left')\n",
        "    return agg_df\n",
        "\n",
        "# Aggregate features for each subset of transactions\n",
        "hist_agg = aggregate_transactions(unauthorized_transactions)\n",
        "auth_agg = aggregate_transactions(authorized_transactions)\n",
        "new_agg = aggregate_transactions(new_transactions)\n",
        "\n",
        "hist_agg.columns = ['hist_' + c if c != 'card_id' else c for c in hist_agg.columns]\n",
        "auth_agg.columns = ['auth_' + c if c != 'card_id' else c for c in auth_agg.columns]\n",
        "new_agg.columns = ['new_' + c if c != 'card_id' else c for c in new_agg.columns]\n",
        "\n",
        "print(hist_agg.head(3))\n",
        "print(f\"Unauthorized transaction features: {hist_agg.shape[1]-1} columns (excluding card_id)\")\n",
        "print(auth_agg.head(3))\n",
        "print(f\"Authorized transaction features: {auth_agg.shape[1]-1} columns\")\n",
        "print(new_agg.head(3))\n",
        "print(f\"New transaction features: {new_agg.shape[1]-1} columns\")\n"
      ],
      "metadata": {
        "id": "QWoRrx4hgUbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the aggregation function above, we specify a dictionary agg_func to calculate numerous features for each card:\n",
        "\n",
        "\n",
        "- For binary flags like authorized_flag (in the context of the full historical data) and category_1, we take sum (count of True occurrences) and mean (percentage of transactions that are True).\n",
        "\n",
        "\n",
        "- For each one-hot encoded category column (those starting with category_2_ or category_3_), we take the mean, which effectively gives the fraction of transactions of that category for the card.\n",
        "\n",
        "\n",
        "- We compute the number of unique values of merchant-related categorical fields (merchant_id, merchant_category_id, state_id, city_id, subsector_id) to capture the diversity of the card‚Äôs transactions.\n",
        "\n",
        "\n",
        "- For continuous/numeric fields like purchase_amount and installments, we gather summary statistics: total sum, mean, max, min, and standard deviation per card.\n",
        "For purchase_month, we get mean, max, min, std to see spending patterns across months of the year.\n",
        "\n",
        "\n",
        "- For purchase_date (converted to a numeric timestamp in seconds), we compute the range (ptp which is max-min), as well as the min and max purchase timestamp for each card (which could indicate the first and last transaction times).\n",
        "\n",
        "- For month_lag and month_diff, we get mean and other stats (for month_lag) to understand recency/spread of transactions.\n",
        "\n",
        "\n",
        "After aggregation, we merge in the total count of transactions (transactions_count) for each card. We then do this for unauthorized historical transactions (hist_agg), authorized transactions (auth_agg), and new merchant transactions (new_agg). We add prefixes to each column name (hist_, auth_, new_) so that features from different sources are distinct when we merge them together. The quick print statements show the first few rows and number of features generated from each source to confirm our aggregation worked as expected."
      ],
      "metadata": {
        "id": "hP3jJvUAgcgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.4 Additional Feature Engineering (Successive Aggregates)**\n",
        "\n",
        "We introduce a set of ‚Äúsuccessive aggregation‚Äù features from the new transactions data to capture interactions between different fields. For example, how the purchase amount varies with certain categorical fields for each card. The idea is to group by card and one field, then aggregate another field within that grouping, and then aggregate across the groups for each card."
      ],
      "metadata": {
        "id": "tcSCjgEQgu2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def successive_aggregates(df, field1, field2):\n",
        "    temp = df.groupby(['card_id', field1])[field2].mean()\n",
        "    temp_df = pd.DataFrame(temp).reset_index()\n",
        "    agg_temp = temp_df.groupby('card_id')[field2].agg(['mean', 'min', 'max', 'std']).reset_index()\n",
        "    agg_temp.columns = [field1 + '_' + field2 + '_' + col for col in agg_temp.columns.values]\n",
        "    agg_temp = agg_temp.rename(columns={agg_temp.columns[0]: 'card_id'})\n",
        "    return agg_temp\n",
        "\n",
        "# Apply successive aggregations on new_transactions for various combinations\n",
        "add_feat1 = successive_aggregates(new_transactions, 'category_1', 'purchase_amount')\n",
        "add_feat2 = successive_aggregates(new_transactions, 'installments', 'purchase_amount')\n",
        "add_feat3 = successive_aggregates(new_transactions, 'city_id', 'purchase_amount')\n",
        "add_feat4 = successive_aggregates(new_transactions, 'category_1', 'installments')\n",
        "\n",
        "# Merge these additional features on card_id\n",
        "additional_fields = add_feat1.merge(add_feat2, on='card_id', how='left')\n",
        "additional_fields = additional_fields.merge(add_feat3, on='card_id', how='left')\n",
        "additional_fields = additional_fields.merge(add_feat4, on='card_id', how='left')\n",
        "\n",
        "print(additional_fields.head(3))\n",
        "print(f\"Additional interaction features from new transactions: {additional_fields.shape[1]-1} columns\")\n"
      ],
      "metadata": {
        "id": "_2nyIJpggtHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define successive_aggregates to capture second-order interactions:\n",
        "\n",
        "\n",
        "- For example, successive_aggregates(new_transactions, 'category_1', 'purchase_amount'): for each card, it finds the average purchase amount for each value of category_1 (which is binary 0/1 in this context, representing some category flag in the card‚Äôs profile), then for that card it computes the mean, min, max, std of those two average values. Essentially, this yields features that describe differences in spending when category_1 is 0 vs 1 for a card.\n",
        "\n",
        "\n",
        "We do similar for 'installments' vs purchase amount (how purchase amount varies by number of installments), 'city_id' vs purchase amount (how spending differs across cities for the card), and 'category_1' vs 'installments' (how installment counts differ by category_1).\n",
        "\n",
        "\n",
        "We then merge all these new interaction features into one DataFrame additional_fields keyed by card_id. These features might help capture behavior nuances in the new merchant transactions that aren‚Äôt directly captured by simple aggregates."
      ],
      "metadata": {
        "id": "j_whgTybg5ui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.5 Merging All Features**\n",
        "\n",
        "Now that we have engineered numerous features from different data sources, we merge them into the training and testing datasets (by card_id). After merging, we perform any final feature selection or reduction, such as dropping highly correlated features that might not add value, to avoid multicollinearity."
      ],
      "metadata": {
        "id": "nhyQrzgWhAJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge all feature tables into train and test dataframes\n",
        "# At this point, train and test contain the card-level base features (feature_1, feature_2, feature_3, first_active_month, elapsed_time)\n",
        "train = train.merge(hist_agg, on='card_id', how='left')\n",
        "test = test.merge(hist_agg, on='card_id', how='left')\n",
        "\n",
        "train = train.merge(auth_agg, on='card_id', how='left')\n",
        "test = test.merge(auth_agg, on='card_id', how='left')\n",
        "\n",
        "train = train.merge(new_agg, on='card_id', how='left')\n",
        "test = test.merge(new_agg, on='card_id', how='left')\n",
        "\n",
        "train = train.merge(agg_auth, on='card_id', how='left')\n",
        "test = test.merge(agg_auth, on='card_id', how='left')\n",
        "\n",
        "train = train.merge(additional_fields, on='card_id', how='left')\n",
        "test = test.merge(additional_fields, on='card_id', how='left')\n",
        "\n",
        "print(f\"Train shape after merging: {train.shape}, Test shape: {test.shape}\")\n"
      ],
      "metadata": {
        "id": "dnb0ZdUTgtD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After merging, the train and test DataFrames now contain all the features we created:\n",
        "\n",
        "- Base card features (feature_1, feature_2, feature_3, elapsed_time, etc.).\n",
        "\n",
        "- Aggregated features from unauthorized historical transactions (prefixed hist_).\n",
        "\n",
        "- Aggregated features from authorized historical transactions (prefixed auth_).\n",
        "\n",
        "- Aggregated features from new merchant transactions (prefixed new_).\n",
        "\n",
        "- The fraction of authorized transactions (authorized_flag_mean).\n",
        "\n",
        "- Additional interaction features from new transactions (like category_1_purchase_amount_mean, etc., from additional_fields).\n",
        "\n",
        "If any features are missing for a card, they will remain NaN after the merge. We should fill these missing values with 0 or another appropriate value for modeling, since a lack of transactions can be represented as zeros in the aggregate features."
      ],
      "metadata": {
        "id": "1cDZJohIhHxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_feature_names(df):\n",
        "    df.columns = [re.sub(r\"[^A-Za-z0-9_]+\", \"_\", col) for col in df.columns]\n",
        "    return df\n",
        "\n",
        "train = clean_feature_names(train)\n",
        "test = clean_feature_names(test)\n",
        "\n",
        "#features = [col for col in train.columns if col not in ['card_id', 'first_active_month']]\n",
        "features = [col for col in train.columns if col not in ['card_id', 'first_active_month', 'target']]\n",
        "categorical_feats = ['feature_2', 'feature_3']\n",
        "\n",
        "train.fillna(0, inplace=True)\n",
        "test.fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "LzBgqF2Bo93Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We replace all NaNs with 0, assuming that absence of data can be treated as no contribution. Now, features contains the list of all feature column names we will use (excluding non-predictive ID or date columns). We also specify categorical_feats for LightGBM ‚Äì in this dataset, feature_2 and feature_3 are known to be categorical codes (even though they are numeric values, they represent categories).\n",
        "\n",
        "**Feature Reduction:**"
      ],
      "metadata": {
        "id": "tnGOdF7VhS1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify and remove highly correlated features\n",
        "corr_matrix = train[features].corr().abs()\n",
        "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "to_drop = [col for col in upper_tri.columns if any(upper_tri[col] > 0.98)]\n",
        "print(f\"Highly correlated features (corr > 0.98) to drop: {to_drop}\")\n",
        "\n",
        "features = [f for f in features if f not in to_drop]"
      ],
      "metadata": {
        "id": "U5OUa7HGhZsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above computes absolute correlations between all feature pairs and finds columns that have any correlation above 0.98 with another. Those identified in to_drop are removed from our feature list."
      ],
      "metadata": {
        "id": "j04xQx5Qhakk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.6 Model Training with LightGBM (5-Fold Cross-Validation)**\n",
        "\n",
        "Now we train a LightGBM model to predict the loyalty score. We use 5-fold cross-validation for reliable estimation of performance, with early stopping in each fold to prevent overfitting. We will also record feature importance from each fold."
      ],
      "metadata": {
        "id": "vYOlqtKehfsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# LightGBM\n",
        "params = {\n",
        "    'num_leaves': 111,\n",
        "    'min_data_in_leaf': 149,\n",
        "    'objective': 'regression',\n",
        "    'max_depth': 9,\n",
        "    'learning_rate': 0.005,\n",
        "    'boosting': 'gbdt',\n",
        "    'feature_fraction': 0.7522,\n",
        "    'bagging_fraction': 0.7083,\n",
        "    'bagging_seed': 11,\n",
        "    'metric': 'rmse',\n",
        "    'lambda_l1': 0.2634,\n",
        "    'random_state': 133,\n",
        "    'verbosity': -1\n",
        "}\n",
        "\n",
        "folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
        "oof_preds = np.zeros(len(train))\n",
        "test_preds = np.zeros(len(test))\n",
        "feature_importance_df = pd.DataFrame()\n",
        "\n",
        "print(\"Starting 5-fold cross-validation for LightGBM...\")\n",
        "for fold, (train_idx, val_idx) in enumerate(folds.split(train, target)):\n",
        "    print(f\"Fold {fold+1}\")\n",
        "\n",
        "    train_data = lgb.Dataset(train.iloc[train_idx][features],\n",
        "                             label=target.iloc[train_idx],\n",
        "                             categorical_feature=categorical_feats,\n",
        "                             free_raw_data=False)\n",
        "\n",
        "    valid_data = lgb.Dataset(train.iloc[val_idx][features],\n",
        "                             label=target.iloc[val_idx],\n",
        "                             categorical_feature=categorical_feats,\n",
        "                             free_raw_data=False)\n",
        "\n",
        "    clf = lgb.train(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=10000,\n",
        "        valid_sets=[train_data, valid_data],\n",
        "        valid_names=['train', 'valid'],\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(200),\n",
        "            lgb.log_evaluation(period=100)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    oof_preds[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
        "    test_fold_pred = clf.predict(test[features], num_iteration=clf.best_iteration)\n",
        "    test_preds += test_fold_pred / folds.n_splits\n",
        "\n",
        "    fold_importance = pd.DataFrame({\n",
        "        'feature': features,\n",
        "        'importance': clf.feature_importance(),\n",
        "        'fold': fold+1\n",
        "    })\n",
        "    feature_importance_df = pd.concat([feature_importance_df, fold_importance], axis=0)\n",
        "\n",
        "    print(f\"Fold {fold+1} RMSE: {mean_squared_error(target.iloc[val_idx], oof_preds[val_idx])**0.5:.4f} \"\n",
        "          f\"(best iteration: {clf.best_iteration})\")\n",
        "\n",
        "cv_rmse = mean_squared_error(target, oof_preds)**0.5\n",
        "print(f\"\\n5-fold CV RMSE: {cv_rmse:.4f}\")"
      ],
      "metadata": {
        "id": "wOMXgrQXiP8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set up LightGBM parameters. The objective is regression (predicting a continuous loyalty score). We use a fairly low learning rate (0.005) and allow up to 10000 boosting rounds with early stopping after 200 rounds of no improvement.\n",
        "\n",
        "We perform 5-fold cross-validation using KFold. For each fold, we create a training set and validation set. We use lgb.Dataset to prepare data, specifying which features are categorical so LightGBM can use them appropriately (with handling of categorical splits).\n",
        "\n",
        "We train the model with lgb.train, including the validation set to monitor RMSE. Early stopping is used to halt training when the validation score stops improving for 200 rounds, which helps prevent overfitting.\n",
        "\n",
        "We store out-of-fold predictions in oof_preds for the validation part of each fold, and we accumulate test set predictions by averaging the predictions from each fold's model.\n",
        "\n",
        "We collect feature importances from each fold into feature_importance_df.\n",
        "After all folds, we calculate the overall cross-validated RMSE on the training data. This gives us an estimate of model performance on unseen data.\n"
      ],
      "metadata": {
        "id": "ihx_RKGMkuIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.7 Feature Importance and Validation Results**\n",
        "\n",
        "visualize the feature importance to see which features the model found most useful. We will average the importance across the folds and plot the top 20 features. We also take a look at the distribution of predictions for the test set to ensure they are in a reasonable range and not wildly extrapolating beyond the training targets."
      ],
      "metadata": {
        "id": "BUAU-c1Skz4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "avg_importance = feature_importance_df.groupby('feature')['importance'].mean().reset_index()\n",
        "avg_importance.sort_values(by='importance', ascending=False, inplace=True)\n",
        "top_feats = avg_importance.head(20)\n",
        "\n",
        "# Plot top 20 feature importances\n",
        "plt.figure(figsize=(8, 10))\n",
        "sns.barplot(x='importance', y='feature', data=top_feats, orient='h')\n",
        "plt.title('Top 20 Feature Importances (LightGBM - avg over folds)')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(test_preds, bins=50, kde=True)\n",
        "plt.title('Distribution of Test Set Predictions')\n",
        "plt.xlabel('Predicted Loyalty Score')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Summary stats of predictions\n",
        "pred_series = pd.Series(test_preds, name=\"Predictions\")\n",
        "print(pred_series.describe())"
      ],
      "metadata": {
        "id": "NtljOMuPk33M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We output a bar chart of the top 20 features by importance. Typically, we expect features derived from purchase amounts and transaction counts to be among the top. For example, one might see features like hist_purchase_amount_sum (total historical spend), new_purchase_amount_sum (total new spend), or hist_month_diff_mean (average recency of historical transactions) as highly important. Categorical card features like feature_2 or feature_3 might also appear if they correlate with loyalty. The distribution of test predictions plot shows how the predicted loyalty scores are spread for the test set. We print summary statistics of the predictions to check for any extreme values."
      ],
      "metadata": {
        "id": "J7lETrmzk7Vn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.8 Hyperparameter Tuning with Optuna**\n",
        "\n",
        "In this section, we perform hyperparameter tuning on the LightGBM model using Optuna. We define a search space for key parameters and use 5-fold cross-validation to evaluate the performance (minimizing RMSE in this case) of each trial. The best parameters found can then be used to retrain the model for improved performance."
      ],
      "metadata": {
        "id": "D0-i1QVFnQq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## THIS IS GOOD BUT IT TAKES FOREVER\n",
        "\n",
        "\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "X = train[features]\n",
        "y = train['target']\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 150),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
        "        'random_state': 42\n",
        "    }\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    rmse_scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        train_data = lgb.Dataset(X.iloc[train_idx], label=y.iloc[train_idx])\n",
        "        valid_data = lgb.Dataset(X.iloc[val_idx], label=y.iloc[val_idx])\n",
        "\n",
        "        model = lgb.train(\n",
        "            params,\n",
        "            train_data,\n",
        "            valid_sets=[valid_data],\n",
        "            num_boost_round=10000,\n",
        "            callbacks=[\n",
        "                lgb.early_stopping(200),\n",
        "                lgb.log_evaluation(period=100)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        preds = model.predict(X.iloc[val_idx], num_iteration=model.best_iteration)\n",
        "        rmse = np.sqrt(mean_squared_error(y.iloc[val_idx], preds))\n",
        "        rmse_scores.append(rmse)\n",
        "\n",
        "    return np.mean(rmse_scores)\n",
        "\n",
        "# Run Optuna Study\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "print(\"Best Params Found by Optuna:\", study.best_params)\n",
        "print(\"Best CV RMSE:\", study.best_value)\n"
      ],
      "metadata": {
        "id": "yLQjymRmu9FI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "X = train[features]\n",
        "y = train['target']\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 12),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
        "        'random_state': 42\n",
        "    }\n",
        "\n",
        "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    rmse_scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        train_data = lgb.Dataset(X.iloc[train_idx], label=y.iloc[train_idx])\n",
        "        valid_data = lgb.Dataset(X.iloc[val_idx], label=y.iloc[val_idx])\n",
        "\n",
        "        model = lgb.train(\n",
        "            params,\n",
        "            train_data,\n",
        "            valid_sets=[valid_data],\n",
        "            num_boost_round=3000,\n",
        "            callbacks=[\n",
        "                lgb.early_stopping(100),\n",
        "                lgb.log_evaluation(period=100)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        preds = model.predict(X.iloc[val_idx], num_iteration=model.best_iteration)\n",
        "        rmse = np.sqrt(mean_squared_error(y.iloc[val_idx], preds))\n",
        "        rmse_scores.append(rmse)\n",
        "\n",
        "    return np.mean(rmse_scores)\n",
        "\n",
        "# Run Optuna Study quicklyTT\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=20, n_jobs=-1)\n",
        "\n",
        "print(\"‚úÖ Best Params Found by Optuna (Quick Version):\", study.best_params)\n",
        "print(\"üìä Best CV RMSE (Quick Version):\", study.best_value)\n"
      ],
      "metadata": {
        "id": "qMsH7M1CzFS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The printed best parameters and RMSE indicate the optimal setting found by Optuna."
      ],
      "metadata": {
        "id": "uohoEeFAvqGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.9 Evaluation Metrics (RMSE, MAE, QWK)**"
      ],
      "metadata": {
        "id": "wno-IL6KvsJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, cohen_kappa_score\n",
        "import numpy as np\n",
        "\n",
        "oof_predictions = oof_preds\n",
        "true_values = target\n",
        "\n",
        "# 1Ô∏èRMSE\n",
        "rmse = np.sqrt(mean_squared_error(true_values, oof_predictions))\n",
        "print(f\"RMSE (Root Mean Squared Error): {rmse:.4f}\")\n",
        "\n",
        "# 2Ô∏èMAE\n",
        "mae = mean_absolute_error(true_values, oof_predictions)\n",
        "print(\" MAE (Mean Absolute Error): {mae:.4f}\")\n",
        "\n",
        "# 3Ô∏èQuadratic Weighted Kappa (QWK)\n",
        "def discretize_preds(preds, bins=5):\n",
        "    return np.digitize(preds, np.linspace(min(preds), max(preds), bins))\n",
        "\n",
        "pred_classes = discretize_preds(oof_predictions, bins=5)\n",
        "true_classes = discretize_preds(true_values, bins=5)\n",
        "\n",
        "qwk = cohen_kappa_score(true_classes, pred_classes, weights='quadratic')\n",
        "print(f\" QWK (Quadratic Weighted Kappa): {qwk:.4f}\")\n"
      ],
      "metadata": {
        "id": "81_r0ZjD0Ibx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lower RMSE/MAE indicates better predictive accuracy (error closer to 0). QWK closer to 1 means the model's predictions have strong agreement with true labels on an ordinal scale. These metrics help validate model consistency across folds and overall."
      ],
      "metadata": {
        "id": "xc8KNhkHwqV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 4: Outcomes**"
      ],
      "metadata": {
        "id": "oG9DY8xLoFqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Subgroups for Analysis\n",
        "\n",
        "To assess our model's accuracy and fairness across different subpopulations, we define three subgroup attributes:\n",
        "\n",
        "- Spending tier: Divide feature_1 (an indicator of cardholder purchasing power) into quintiles (5 tiers, from Tier1 = lowest spend to Tier5 = highest spend). This groups cardholders by overall spending level.\n",
        "\n",
        "- Recency: Categorize hist_month_lag_mean (average transaction recency) into three buckets: 'older' for values ‚â§ -2 (transactions mostly in the distant past), 'recent' for value = -1, and 'current' for values ‚â• 0 (mostly recent transactions). This captures how recent or stale each card's transaction history is.\n",
        "\n",
        "- Authorization status: Classify authorized_flag_mean (fraction of transactions approved) into HighAuth (‚â• 0.5) and LowAuth (< 0.5). HighAuth cards had a majority of transactions authorized, whereas LowAuth cards saw more declines, indicating different usage behaviors.\n",
        "\n",
        "\n",
        "**Prepare train_results DataFrame with Predictions and Subgroup Labels**\n",
        "\n",
        "We will create a DataFrame train_results that contains each training example‚Äôs actual target (loyalty_score), the model‚Äôs predicted score (our out-of-fold predictions), and the subgroup labels defined above."
      ],
      "metadata": {
        "id": "qNwbj0jJIsN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_results = pd.DataFrame({\n",
        "    'card_id': train['card_id'],\n",
        "    'actual': target,\n",
        "    'prediction': oof_preds\n",
        "})"
      ],
      "metadata": {
        "id": "XXuig2xg1vF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if historical_transactions['authorized_flag'].dtype == object:\n",
        "    historical_transactions['authorized_flag'] = historical_transactions['authorized_flag'].map({'Y': 1, 'N': 0}).astype(np.int8)\n",
        "\n",
        "# month_diff\n",
        "import datetime\n",
        "historical_transactions['month_diff'] = (\n",
        "    (datetime.datetime.today() - historical_transactions['purchase_date']).dt.days // 30\n",
        "    + historical_transactions['month_lag']\n",
        ")\n",
        "\n",
        "# agg\n",
        "agg = (\n",
        "    historical_transactions\n",
        "    .groupby('card_id', as_index=False)\n",
        "    .agg({\n",
        "        'month_diff': 'mean',\n",
        "        'authorized_flag': 'mean'\n",
        "    })\n",
        "    .rename(columns={'month_diff': 'hist_month_diff_mean', 'authorized_flag': 'authorized_flag_mean'})\n",
        ")\n",
        "\n",
        "train_results = train_results.merge(agg, on='card_id', how='left')\n",
        "\n",
        "print(train_results[['hist_month_diff_mean', 'authorized_flag_mean']].head())\n"
      ],
      "metadata": {
        "id": "w2CTFJNS1jdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_results.columns)\n",
        "print(train.columns)"
      ],
      "metadata": {
        "id": "tPCegolk23t2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Spending tier (quintiles of hist_month_diff_mean)\n",
        "tmp = pd.qcut(train_results['hist_month_diff_mean'], q=5, duplicates='drop')\n",
        "labels = [f\"Tier{i}\" for i in range(1, tmp.cat.categories.size + 1)]\n",
        "train_results['spending_tier'] = pd.qcut(\n",
        "    train_results['hist_month_diff_mean'], q=5, labels=labels, duplicates='drop'\n",
        ")\n",
        "# 2. Recency from hist_month_diff_mean\n",
        "rec_rounded = np.round(train_results['hist_month_diff_mean']).clip(-2, 0)\n",
        "train_results['recency'] = rec_rounded.map({-2:'older', -1:'recent', 0:'current'})\n",
        "\n",
        "# 3. Authorization status\n",
        "train_results['auth_status'] = np.where(\n",
        "    train_results['authorized_flag_mean'] >= 0.5, 'HighAuth', 'LowAuth'\n",
        ")"
      ],
      "metadata": {
        "id": "GJzjFvMVE0es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(train_results[['spending_tier', 'recency', 'auth_status']].agg(['count', 'nunique']))"
      ],
      "metadata": {
        "id": "K0f0KppzH_Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above adds three new columns to train_results. We use pd.qcut to create approximately equal-sized quintile groups for feature_1. We then map the recency and authorization features to descriptive categorical labels. Finally, we display the count of instances and number of unique categories in each subgroup to ensure the grouping is as expected."
      ],
      "metadata": {
        "id": "qR2f01S8I-R5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy Metrics by Subgroup**\n",
        "\n",
        "Next, we evaluate model performance within each subgroup using two common accuracy metrics: Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE).\n",
        "\n",
        "Lower values indicate better predictions. We compute RMSE and MAE for each category of each subgroup, allowing us to compare errors across groups:"
      ],
      "metadata": {
        "id": "Nee56dHTJB4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "rmse = lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "# Compute RMSE and MAE per subgroup category\n",
        "for col in ['spending_tier', 'recency', 'auth_status']:\n",
        "    metrics_by_group = train_results.groupby(col).apply(\n",
        "        lambda df: pd.Series({\n",
        "            'RMSE': rmse(df['actual'], df['prediction']),\n",
        "            'MAE': mean_absolute_error(df['actual'], df['prediction'])\n",
        "        })\n",
        "    )\n",
        "    print(f\"\\nPerformance metrics by {col}:\")\n",
        "    display(metrics_by_group)\n"
      ],
      "metadata": {
        "id": "rhwLfLibICCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bootstrapped Confidence Intervals**\n",
        "\n",
        "To gauge the uncertainty in these error estimates, we can compute bootstrapped 95% confidence intervals for the RMSE and MAE in each subgroup."
      ],
      "metadata": {
        "id": "CXBlHzWAJJnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def bootstrap_ci(y_true, y_pred, metric_func, n_bootstrap=300):\n",
        "    \"\"\"Compute bootstrap 95% CI for a given metric.\"\"\"\n",
        "    stats = []\n",
        "    n = len(y_true)\n",
        "    for i in range(n_bootstrap):\n",
        "        idx = np.random.randint(0, n, n)\n",
        "        stats.append(metric_func(y_true.iloc[idx], y_pred.iloc[idx]))\n",
        "    # 2.5th and 97.5th percentiles for 95% CI\n",
        "    return np.percentile(stats, [2.5, 97.5])\n",
        "\n",
        "for col in ['spending_tier', 'recency', 'auth_status']:\n",
        "    print(f\"\\n95% bootstrap CI for metrics by {col}:\")\n",
        "    for group_val, group_df in train_results.groupby(col):\n",
        "        rmse_lower, rmse_upper = bootstrap_ci(group_df['actual'], group_df['prediction'], rmse)\n",
        "        mae_lower, mae_upper = bootstrap_ci(group_df['actual'], group_df['prediction'], mean_absolute_error)\n",
        "        print(f\"  {col} = {group_val}:  RMSE 95% CI = [{rmse_lower:.3f}, {rmse_upper:.3f}],  MAE 95% CI = [{mae_lower:.3f}, {mae_upper:.3f}]\")"
      ],
      "metadata": {
        "id": "0FY4EppcIFYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Set Prediction Distribution**\n",
        "\n",
        "It's also informative to examine the distribution of the model's predictions on the test set. This helps verify that the predictions fall in a sensible range and to compare with training target distribution for any shifts. Below, we visualize the distribution of test_preds:"
      ],
      "metadata": {
        "id": "AvYXOMg6JTuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(test_preds, bins=50, color='steelblue', edgecolor='black')\n",
        "plt.title(\"Distribution of Test Set Predictions\")\n",
        "plt.xlabel(\"Predicted loyalty score\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hZddnEptILkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Explainability with SHAP**\n",
        "\n",
        "This section uses SHAP (SHapley Additive exPlanations) to interpret the trained LightGBM model. We compute SHAP values to understand feature importance and contributions. First, a global interpretation is provided via a summary plot (showing which features have the highest impact on model output across all samples). Then, we provide a local interpretation for an individual prediction using a force plot (to see how each feature influenced that specific prediction)."
      ],
      "metadata": {
        "id": "XTQpwtgs3bT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_to_explain = clf  # Use the final trained LightGBM model"
      ],
      "metadata": {
        "id": "j_nBXLCO4Yzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "# Use the final trained LightGBM model for SHAP analysis\n",
        "model_to_explain = clf\n",
        "\n",
        "explainer = shap.TreeExplainer(model_to_explain)\n",
        "X_train_sample = train[features]\n",
        "\n",
        "if len(X_train_sample) > 10000:\n",
        "    X_train_sample = X_train_sample.sample(10000, random_state=42)  # Sample for faster computation\n",
        "\n",
        "shap_values = explainer.shap_values(X_train_sample)\n",
        "\n",
        "# Global interpretation: Feature Importance Bar Plot\n",
        "shap.summary_plot(shap_values, X_train_sample, plot_type=\"bar\")\n",
        "\n",
        "# Detailed global interpretation: SHAP Summary Plot\n",
        "shap.summary_plot(shap_values, X_train_sample)\n",
        "\n",
        "# Local interpretation: Explain an individual prediction\n",
        "shap.initjs()\n",
        "sample_index = 0  # Change this to select different samples\n",
        "sample_features = X_train_sample.iloc[sample_index]\n",
        "sample_shap_values = shap_values[sample_index]\n",
        "\n",
        "shap.force_plot(explainer.expected_value, sample_shap_values, sample_features)"
      ],
      "metadata": {
        "id": "Rfjeyuj33fK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the SHAP bar plot, features with longer bars are more influential overall. The summary dot plot provides insight into how each feature‚Äôs value (color) correlates with impact on the prediction (SHAP value on the x-axis). In the force plot for a single instance, features pushing the prediction higher (positive SHAP) are shown in red, and those pushing it lower are in blue, giving a clear explanation for that individual prediction."
      ],
      "metadata": {
        "id": "3KHiT8n93k-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Local Explanation with LIME**"
      ],
      "metadata": {
        "id": "3pmXt8Bi3v_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime\n",
        "\n",
        "import numpy as np\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "X_train_array = train[features].values\n",
        "feature_names = list(features)\n",
        "class_names = ['target']\n",
        "\n",
        "explainer = LimeTabularExplainer(\n",
        "    X_train_array,\n",
        "    mode='regression',\n",
        "    feature_names=feature_names,\n",
        "    class_names=class_names,\n",
        "    discretize_continuous=True\n",
        ")\n",
        "\n",
        "instance_idx = 0\n",
        "instance = test[features].iloc[instance_idx].values\n",
        "\n",
        "exp = explainer.explain_instance(instance, model_to_explain.predict, num_features=10)\n",
        "\n",
        "print(f\"LIME explanation for test instance {instance_idx}:\")\n",
        "for feature, contribution in exp.as_list():\n",
        "    print(f\"  {feature}: {contribution:.3f}\")"
      ],
      "metadata": {
        "id": "0x3VGSt73-ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp.show_in_notebook(show_table=True)"
      ],
      "metadata": {
        "id": "tZxeTtBC6SKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 5**\n",
        "\n",
        "Finally, we compile our insights and evaluate the overall ADS solution in terms of robustness, fairness, and practical implications.\n",
        "\n",
        "**Model Performance and Validation**"
      ],
      "metadata": {
        "id": "yMfxJbb_IR8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "overall_cv_rmse = np.sqrt(mean_squared_error(train_results['actual'], train_results['prediction']))\n",
        "print(f\"Overall CV RMSE: {overall_cv_rmse:.4f}\")"
      ],
      "metadata": {
        "id": "lzn5bE0CIPLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Quadratic Weighted Kappa (QWK)**"
      ],
      "metadata": {
        "id": "R0aYLYuB6jJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "actual_bins = pd.qcut(train_results['actual'], q=5, labels=False)\n",
        "pred_bins = pd.qcut(train_results['prediction'], q=5, labels=False)\n",
        "\n",
        "qwk = cohen_kappa_score(actual_bins, pred_bins, weights='quadratic')\n",
        "print(f\"Overall CV QWK: {qwk:.4f}\")\n"
      ],
      "metadata": {
        "id": "b3oAupW34Tny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Robustness Analysis**"
      ],
      "metadata": {
        "id": "rVVjUzHo6qR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group_metrics = train_results.groupby('spending_tier').apply(\n",
        "    lambda df: np.sqrt(mean_squared_error(df['actual'], df['prediction']))\n",
        ")\n",
        "print(group_metrics)"
      ],
      "metadata": {
        "id": "Opi9wR774TlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZUFtF5Sc4TiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F3u6tD8rlL1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WVM__4j8lLx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qkzyc52zgs17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1U_69tXwgsxp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}